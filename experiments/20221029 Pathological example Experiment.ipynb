{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68db98ef",
   "metadata": {},
   "source": [
    "# Pathological example Experiment\n",
    "This notebook is aimed at testing the capabilities of the class differentiation measure for the case of a pathological example dataset classification problem. Also, in this notebook, it is intended to test the capabilities of the new intersection measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67f402",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8d05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3ee0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magics first (server issues)\n",
    "\n",
    "# if you want static matplotlib plot :\n",
    "#%matplotlib inline \n",
    "# Do below if you want interactive matplotlib plot :\n",
    "%matplotlib notebook \n",
    "\n",
    "# https://ipython.org/ipython-doc/dev/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#Customisations\n",
    "\n",
    "\n",
    "#Any tweaks that normally go in .matplotlibrc, etc. should explicity go here\n",
    "plt.rcParams['figure.figsize'] = (7,5)\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060eb0b",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e32b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_point(point,array):\n",
    "    dist = []\n",
    "    for i in range(array.shape[0]):\n",
    "        d = np.linalg.norm(point-array[i])\n",
    "        dist.append(d)\n",
    "    return np.array(dist)\n",
    "\n",
    "def distance_factor(sample,mode = 0):\n",
    "    \"\"\"\n",
    "    This function computes the relative contrast provided by the paper  \"ON the suprising behaviour\". \n",
    "    In this case, the relative constras will be computed with respect the mean.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    sample: Dataset from which the relative factor must be computed.\n",
    "    mode: working mode. Default:0\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    rt : Relative contrast.\n",
    "    \"\"\"\n",
    "    if mode == 0:\n",
    "        dt = centroid_distance(sample)\n",
    "        m = np.min(dt)\n",
    "        M = np.max(dt)\n",
    "        rt = (M-m)/m\n",
    "        return rt\n",
    "    \n",
    "\n",
    "def mahalanobis(array,observations):\n",
    "    \"\"\"\n",
    "    This function computes the Mahalanobis \"Norm\" of a provided vector array and a provided set of\n",
    "     observations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array: Provided array. This distance will provide a measure of how much this vector belongs to\n",
    "    the provided set of vectors.\n",
    "    observations: Set of previous observations.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Mahalanobis distance to the provided set.\n",
    "    \n",
    "    \"\"\"\n",
    "    observations = np.transpose(observations)\n",
    "    #print(f'Observations shape: {observations.shape}')\n",
    "    mean = np.mean(observations,axis = 1)\n",
    "    #print(f'Mean  {mean}')\n",
    "    cov = np.cov(observations)\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    #print(f'Cov: {cov}')\n",
    "    v = array - mean\n",
    "    distance = np.sqrt(np.matmul(np.matmul(v,inv_cov),v))\n",
    "    return distance\n",
    "\n",
    "def d_mahalanobis_distance(sample):\n",
    "    \"\"\"\n",
    "    This function computes the mahalanobis distance of each single point to itself\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample. samples or dataset used to compute the mahalanobis distance.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    d_v: distance vector\n",
    "    \"\"\"\n",
    "    d_v = []\n",
    "    for i in sample:\n",
    "        d_v.append(mahalanobis(i,sample))\n",
    "    d_v = np.array(d_v)\n",
    "    return d_v\n",
    "\n",
    "def centroid_distance(sample):\n",
    "    \"\"\"\n",
    "    This function computes the distance from the mass center of a given set to each other point\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    sample: sample of elements on which the method will be applied.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    d_v: distance vector\n",
    "    \"\"\"\n",
    "    m = np.mean(sample,axis=0)\n",
    "    d_v=distance_to_point(m,sample)\n",
    "    return d_v\n",
    "\n",
    "def density_funct_estimation(sample,density_factor=100):\n",
    "    \"\"\"\n",
    "    This function computes the density estimation associated to a category in a\n",
    "    dataset.\n",
    "    \"\"\"\n",
    "    q = []\n",
    "    for i in range(1,density_factor):\n",
    "        q.append(np.quantile(sample,i/density_factor))\n",
    "    q = np.array(q)\n",
    "    q = np.concatenate((np.array([np.min(sample)]),q,np.array([np.max(sample)])))\n",
    "    dq = np.diff(q)\n",
    "    h = (1/density_factor)/dq\n",
    "\n",
    "    return q[1:],h\n",
    "\n",
    "def quantile_entropy(density_vector):\n",
    "    \"\"\"\n",
    "    This function computes the entropy associated to a simple density function made out of constant values\n",
    "    obtained for equally spaciated quantiles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    density_vector: array. Vector containing the densities obtained for a specific sample.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    qe: Scalar value associated to the entropy of the sample.\n",
    "    \n",
    "    \"\"\"\n",
    "    c = 1/density_vector.shape[0]\n",
    "    qe = (-c)*np.sum(np.log(density_vector))\n",
    "    return qe\n",
    "def n_sphere_vol(r,dim = 2):\n",
    "    if dim%2 == 0:\n",
    "        k = dim/2\n",
    "        volume = ((np.pi**k)/np.math.factorial(k))*r**dim\n",
    "    elif dim%2 == 1:\n",
    "        k = dim//2\n",
    "        volume = ((2*np.math.factorial(k)*(4*np.pi)**k)/(np.math.factorial(dim)))*r**dim     \n",
    "    return volume\n",
    "\n",
    "def mutual_density(target_random_sample, non_target_random_sample,non_target_quantiles ):\n",
    "    \"\"\"\n",
    "    This function computes the empirical probability measure of the intersection of two \n",
    "    categories within a dataset.\n",
    "    \n",
    "    Params\n",
    "    --------\n",
    "    target_random_sample: np.array. Samples that belongs to the target category. The\n",
    "                          probability measure will be computed with respect the empirical\n",
    "                          probability distribution associated to this category.\n",
    "    non_target_random_sample: np.array. Samples that belong to some other category. If \n",
    "                          these two categories are not disjoint then the intersection will\n",
    "                          be a measurable set and its measure can be computed with respect\n",
    "                          the different probability measures.\n",
    "    non_target_quantiles: np.array. Array that contains the quantiles of distances with respect\n",
    "                          the non_target_random_sample.\n",
    "                          \n",
    "    Return\n",
    "    -------\n",
    "    m: float. Measure of the intersection of the two categories with respect the empirical prob\n",
    "              bability measure defined for the target_random_sample-\n",
    "    \"\"\"\n",
    "    \n",
    "    r = non_target_quantiles[-1]\n",
    "    c2 = np.mean(non_target_random_sample,axis=0)\n",
    "    distances_to_c2 = distance_to_point(point = c2,array = target_random_sample)\n",
    "    categories_intersection = distances_to_c2[np.where(distances_to_c2<=r)]\n",
    "    m = categories_intersection.shape[0]/target_random_sample.shape[0]\n",
    "    return m\n",
    "\n",
    "def intersection_divergence_entropy(dataset_1,dataset_2):\n",
    "    \"\"\"\n",
    "    Test\n",
    "    \"\"\"\n",
    "    distances1 = centroid_distance(dataset_1)\n",
    "    distances2 = centroid_distance(dataset_2)\n",
    "    q1,h1 = density_funct_estimation(distances1)\n",
    "    q2,h2 = density_funct_estimation(distances2)\n",
    "    m1 = mutual_density(dataset_1,dataset_2,q2)\n",
    "    m2 = mutual_density(dataset_2,dataset_1,q1)\n",
    "    i_d_e = m1*np.log(m1/m2)\n",
    "    return i_d_e\n",
    "\n",
    "def mutual_density_divergence(target_category, category_list):\n",
    "    \"\"\"\n",
    "    This function computes a coefficient associated to the measure of\n",
    "    the intersections of a measurable set with respect someother measurable sets \n",
    "    for a set of probability measures. This coefficient intends to provide insights \n",
    "    of the average 'size'of these intersections and, in that way, how distinguishable\n",
    "    is a set from others.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    target_category: array. Dataset associated to the target category for which the coe-\n",
    "                     fficient will be computed.\n",
    "    category_list: list of arrays. This list must contain all datasets associated to the \n",
    "              different categories  in a classification problem.\n",
    "    mode: int. Used mean method. Default:0 Arithmetic mean. 1, geometric mean.\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    phi: float. Coefficient associated to the target category.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    relative_measures = []\n",
    "    for category in category_list:\n",
    "        d = centroid_distance(category)\n",
    "        q,h = density_funct_estimation(d)\n",
    "        m=mutual_density(target_category,category,q)\n",
    "        relative_measures.append(m)\n",
    "    relative_measures = np.array(relative_measures)\n",
    "    phi = np.mean(np.log(1/(1-relative_measures)))\n",
    "    return phi, relative_measures\n",
    "\n",
    "def dataset_density_function_estimation(dataset):\n",
    "    d = centroid_distance(dataset)\n",
    "    q,h = density_funct_estimation(d)\n",
    "    return q,h\n",
    "\n",
    "def dataset_quantile_entropy(dataset):\n",
    "    q,h = dataset_density_function_estimation(dataset)\n",
    "    e = quantile_entropy(h)\n",
    "    return e\n",
    "def mean_categorical_entropy(category_list):\n",
    "    \"\"\"\n",
    "    This function computes the mean categorical entropy defined for n categories in\n",
    "    a classification problem as (phi1*h1+...+phin*hn)/n\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    category_list:list of arrays. This list must contain all datasets associated to the \n",
    "              different categories  in a classification problem.\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    mce:float. Mean Cathegorical Entropy. \n",
    "    \"\"\"\n",
    "    entropies = []\n",
    "    mdd_coefficients=[]\n",
    "    \n",
    "    for i in  range(category_list.shape[0]):\n",
    "        quantile_entropy = dataset_quantile_entropy(category_list[i])\n",
    "        entropies.append(quantile_entropy)\n",
    "        target_category = category_list[i]\n",
    "        non_target_category = np.delete(category_list,i,axis = 0)\n",
    "        #print(f'shape {non_target_category.shape}')\n",
    "        phi,_relative_measures = mutual_density_divergence(target_category = target_category,category_list = non_target_category)\n",
    "        print(f'Relative Measures {_relative_measures}')\n",
    "        print(f'Coefficients {phi}')\n",
    "        mdd_coefficients.append(phi)\n",
    "    entropies = np.array(entropies)\n",
    "    mdd_coefficients = np.array(mdd_coefficients)    \n",
    "    mce = np.mean(100*np.log2(mdd_coefficients)+entropies)\n",
    "    return mce\n",
    "\n",
    "\n",
    "\n",
    "def category_finder(training_dataset,category_column):\n",
    "    \"\"\"\n",
    "    This function takes a pandas DataFrame training_dataset as \n",
    "    input and returns a set of pandas DataFrame each associated \n",
    "    to a specific category\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_dataset: Pandas DataFrame. DataFrame that contains the\n",
    "                      training dataset\n",
    "    category_columns: str. Name of the column that contains the \n",
    "    categories\n",
    "    \n",
    "    Returns: category_dict. Dictionary that contains the different\n",
    "            categories dataframes\n",
    "    \"\"\"\n",
    "    category_dict = {}\n",
    "    categories = training_dataset[category_column].unique()\n",
    "    #print(f'Unique Values: {categories}')\n",
    "    for i in range(categories.shape[0]):\n",
    "        #print(f'Categoria: {categories[i]}')\n",
    "        name = categories[i]\n",
    "        current_df = training_dataset.loc[training_dataset[category_column]==categories[i]].copy()\n",
    "        category_dict[name]= current_df\n",
    "    return category_dict\n",
    "\n",
    "def mean_mutual_density_divergence(dataset,representation_columns,category_column):\n",
    "    \"\"\"\n",
    "    this function computes the mean mutual density divergence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: pandas DataFrame. dataset that is going to be used for training the model (\n",
    "             all columns must be included)\n",
    "    representation_columns: list of str. Names of the columns that stand for the representation,\n",
    "                          i.e. the feature columns\n",
    "    category_column: str. Name of the column that contains the categories\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    mean_phi: float. Mean of the phi values associated to the different mutual density divergence\n",
    "    \n",
    "    phis: list of float. List of phi values associated to the different mutual density divergence\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    categories = category_finder(dataset,category_column)\n",
    "    categories_names = list(categories.keys())\n",
    "    phis = []\n",
    "    for i in range(len(categories_names)):\n",
    "        remaining_category_names = categories_names.copy()\n",
    "        target_category_name = categories_names[i]\n",
    "        remaining_category_names.remove(target_category_name)\n",
    "        #print(f'Target Category Name: {target_category_name}')\n",
    "        #print(f'Remaining Category Name: {remaining_category_names}')\n",
    "        #computing the divergence\n",
    "        target_X = categories[target_category_name][representation_columns].values\n",
    "        #print(f'This is target_X: { target_X}')\n",
    "        complementary_targets = [categories[name][representation_columns].values for name in remaining_category_names]\n",
    "        #print(f'These are complementary targets: {complementary_targets}')\n",
    "        \n",
    "        phi,rm = mutual_density_divergence(target_category=target_X, category_list=complementary_targets)\n",
    "        phis.append(phi)\n",
    "        #print(f'Mean phi:{np.mean(phis)}\\nList of phis: {phis}')\n",
    "        \n",
    "        del remaining_category_names\n",
    "    return np.mean(phis),phis\n",
    "\n",
    "def average_inner_distance(target_points, data):\n",
    "    \"\"\"\n",
    "    This function compute the average distance from a representation\n",
    "    of a set called data to a series of target points.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target_points: np.array. Array of target points. These points are \n",
    "                   the reference points for computing the average distance.\n",
    "    data: np.array. Array associated to the data points for a especific \n",
    "                    representation.\n",
    "                    \n",
    "    Return\n",
    "    ------\n",
    "    mean_dist: float. Mean distance to the different points\n",
    "    std_dist:  float. Standard deviation of the distances.\n",
    "    \"\"\"\n",
    "    distances_array = np.zeros(shape=(target_points.shape[0],data.shape[0],target_points.shape[1]))\n",
    "    for i in range(target_points.shape[0]):\n",
    "        distances_array[i]= target_points[i] - data\n",
    "    d = np.linalg.norm(distances_array,axis = 2)\n",
    "    m_d = np.min(d,axis = 1)\n",
    "    mean_dist = np.mean(m_d)\n",
    "    std_dist = np.std(m_d)\n",
    "    return mean_dist, std_dist\n",
    "    \n",
    "    \n",
    "def dataset_intersection_counter(target_reference_points, radius, target_dataset):\n",
    "    \"\"\"\n",
    "    This function counts the number of points of a target dataset inside the covering\n",
    "    defined by the balls of centers in target_reference_points (coming from a reference dataset)\n",
    "    with radius radius.\n",
    "    The idea is to detect if two given datasets the intersectionis void or not.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target_reference_points: np.array. Array of target points. These points come from the targe dataset of\n",
    "                   reference.\n",
    "    radius: float. Radius of the balls\n",
    "    target_dataset: np.array. Array of points associated to someother dataset whose intersection \n",
    "    with the reference dataset wants to be computed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    intersection_measure: float. This value is the number of target reference points that have points \n",
    "                          of target dataset inside a ball of center a target_point and radius radius.\n",
    "    \"\"\"\n",
    "    distances_array = np.zeros(shape=(target_reference_points.shape[0],target_dataset.shape[0],target_reference_points.shape[1]))\n",
    "    for i in range(target_reference_points.shape[0]):\n",
    "        distances_array[i]= target_reference_points[i] - target_dataset\n",
    "    d = np.linalg.norm(distances_array,axis = 2)\n",
    "    n_intersection_indexes = np.unique(np.where(d <= radius)[0]).shape[0]\n",
    "    intersection_measure = n_intersection_indexes/target_reference_points.shape[0]\n",
    "    # provisional\n",
    "    if intersection_measure > 0.95:\n",
    "        intersection_measure = 1\n",
    "    return intersection_measure\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d16d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_density_divergence_2(target_category, category_list):\n",
    "    \"\"\"\n",
    "    This function computes a coefficient associated to the measure of\n",
    "    the intersections of a measurable set with respect someother measurable sets \n",
    "    for a set of probability measures. This coefficient intends to provide insights \n",
    "    of the average 'size'of these intersections and, in that way, how distinguishable\n",
    "    is a set from others.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    target_category: array. Dataset associated to the target category for which the coe-\n",
    "                     fficient will be computed.\n",
    "    category_list: list of arrays. This list must contain all datasets associated to the \n",
    "              different categories  in a classification problem.\n",
    "    mode: int. Used mean method. Default:0 Arithmetic mean. 1, geometric mean.\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    phi: float. Coefficient associated to the target category.\n",
    "    \n",
    "    \"\"\"\n",
    "    cpr = 0.2#Control Points Rate\n",
    "    relative_measures = []\n",
    "    control_points = target_category[:int(cpr*target_category.shape[0])]\n",
    "    reference_data = target_category[int(cpr*target_category.shape[0]):]\n",
    "    mn,s = average_inner_distance(target_points = control_points, data = reference_data)\n",
    "    for category in category_list:\n",
    "        m = dataset_intersection_counter(target_reference_points = control_points, radius = mn+2*s, target_dataset=category)\n",
    "        relative_measures.append(m)\n",
    "    relative_measures = np.array(relative_measures)\n",
    "    phi = np.mean(np.log(1/(1-relative_measures)))\n",
    "    return phi, relative_measures\n",
    "\n",
    "def mean_mutual_density_divergence_2(dataset,representation_columns,category_column):\n",
    "    \"\"\"\n",
    "    this function computes the mean mutual density divergence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: pandas DataFrame. dataset that is going to be used for training the model (\n",
    "             all columns must be included)\n",
    "    representation_columns: list of str. Names of the columns that stand for the representation,\n",
    "                          i.e. the feature columns\n",
    "    category_column: str. Name of the column that contains the categories\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    mean_phi: float. Mean of the phi values associated to the different mutual density divergence\n",
    "    \n",
    "    phis: list of float. List of phi values associated to the different mutual density divergence\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    categories = category_finder(dataset,category_column)\n",
    "    categories_names = list(categories.keys())\n",
    "    phis = []\n",
    "    for i in range(len(categories_names)):\n",
    "        remaining_category_names = categories_names.copy()\n",
    "        target_category_name = categories_names[i]\n",
    "        remaining_category_names.remove(target_category_name)\n",
    "        #print(f'Target Category Name: {target_category_name}')\n",
    "        #print(f'Remaining Category Name: {remaining_category_names}')\n",
    "        #computing the divergence\n",
    "        target_X = categories[target_category_name][representation_columns].values\n",
    "        #print(f'This is target_X: { target_X}')\n",
    "        complementary_targets = [categories[name][representation_columns].values for name in remaining_category_names]\n",
    "        #print(f'These are complementary targets: {complementary_targets}')\n",
    "        \n",
    "        phi,rm = mutual_density_divergence_2(target_category=target_X, category_list=complementary_targets)\n",
    "        phis.append(phi)\n",
    "        print(f'Relative measures: {rm}')\n",
    "        #print(f'Mean phi:{np.mean(phis)}\\nList of phis: {phis}')\n",
    "        \n",
    "        del remaining_category_names\n",
    "    return np.mean(phis),phis\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba665f4",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c46fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = Path('./datasets/cell_phone_train.csv')\n",
    "#p_test = Path('./datasets/cell_phone_test.csv')\n",
    "scaler = MinMaxScaler()\n",
    "#columns = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b63025",
   "metadata": {},
   "source": [
    "### Control Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d717c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature_1  feature_2  feature_3  category\n",
      "2598   0.857260   0.446257        0.0       1.0\n",
      "2004   0.576564   0.143983        0.0       1.0\n",
      "1573   0.354246   0.601952        0.0       1.0\n",
      "2770   0.831743   0.770601        0.0       1.0\n",
      "2748   0.527238   0.017458        0.0       1.0\n",
      "Data Shape: (3000, 4)\n"
     ]
    }
   ],
   "source": [
    "#Dataset generation\n",
    "h = 0\n",
    "s1 = 1\n",
    "s2 = 1\n",
    "z1 = np.zeros(shape=(1500,3))\n",
    "z2 = np.zeros(shape=(1500,3))\n",
    "z2[:,2] = h\n",
    "cat1 = s1*np.random.random(size=(1500,2))\n",
    "cat2 = s2*np.random.random(size=(1500,2)) \n",
    "z1[:,[0,1]]=cat1\n",
    "z2[:,[0,1]] = cat2\n",
    "x = np.concatenate((z1,z2),axis = 0)\n",
    "y = np.concatenate((np.zeros(1500),np.ones(1500)))\n",
    "# Dumping into dataframe\n",
    "data = pd.DataFrame()\n",
    "data['feature_1']=x[:,0]\n",
    "data['feature_2']=x[:,1]\n",
    "data['feature_3']=x[:,2]\n",
    "data['category']=y\n",
    "data = data.sample(frac=1)\n",
    "print(data.head())\n",
    "columns = list(data.columns)[:-1]\n",
    "target_column = list(data.columns)[-1]\n",
    "print(f'Data Shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22b93cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    1500\n",
       "0.0    1500\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7135f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaled_data = data.copy()\n",
    "scaled_data[columns] = scaler.fit_transform(scaled_data[columns])\n",
    "X = scaled_data[columns]\n",
    "Y = scaled_data[target_column]\n",
    "t_r = 0.7\n",
    "\n",
    "X_training = X.iloc[:int(t_r*X.shape[0])]\n",
    "X_test = X.iloc[int(t_r*X.shape[0]):]\n",
    "Y_training = Y.iloc[:int(t_r*Y.shape[0])]\n",
    "Y_test = Y.iloc[int(t_r*Y.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9517541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:       feature_1  feature_2  feature_3  category\n",
      "2598   0.857260   0.446257        0.0       1.0\n",
      "2004   0.576564   0.143983        0.0       1.0\n",
      "1573   0.354246   0.601952        0.0       1.0\n",
      "2770   0.831743   0.770601        0.0       1.0\n",
      "2748   0.527238   0.017458        0.0       1.0\n",
      "X:      feature_1  feature_2  feature_3\n",
      "2598   0.857258   0.446147        0.0\n",
      "2004   0.576356   0.143710        0.0\n",
      "1573   0.353875   0.601926        0.0\n",
      "2770   0.831722   0.770666        0.0\n",
      "2748   0.526994   0.017116        0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Data: {data.head()}\\nX:{X.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad7f8b",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cde04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(32,64,64,16),random_state=1, max_iter=200)\n",
    "gbc = GradientBoostingClassifier(n_estimators=150, learning_rate=1.0,max_depth=1, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da130493",
   "metadata": {},
   "source": [
    "### Control Dataset information analysis. Old intersection measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb31287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "---------------------------------\n",
      "Mean phi: inf\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "Phi list: [inf, 6.243195555274128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8691/985750278.py:204: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  phi = np.mean(np.log(1/(1-relative_measures)))\n"
     ]
    }
   ],
   "source": [
    "mphi,phis = mean_mutual_density_divergence(data.iloc[:int(t_r*data.shape[0])],columns,target_column)\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Mean phi: {mphi}')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Phi list: {phis}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f4b43",
   "metadata": {},
   "source": [
    "### Control Dataset information analysis. New intersection measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "165eddbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative measures: [1]\n",
      "Relative measures: [1]\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "Mean phi: inf\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "Phi list: [inf, inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8691/175728479.py:31: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  phi = np.mean(np.log(1/(1-relative_measures)))\n"
     ]
    }
   ],
   "source": [
    "mphi,phis = mean_mutual_density_divergence_2(data.iloc[:int(t_r*data.shape[0])],columns,target_column)\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Mean phi: {mphi}')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Phi list: {phis}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118c640",
   "metadata": {},
   "source": [
    "### Model Training on Control Dataset with old intersection measure\n",
    "#### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "313aba33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(32, 64, 64, 16), random_state=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_training,Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03640103",
   "metadata": {},
   "source": [
    "#### Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1c3b83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=1.0, max_depth=1, n_estimators=150,\n",
       "                           random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.fit(X_training,Y_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa0eaf",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c75036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean phi: inf\n",
      "Phi list: [inf, inf]\n",
      "Scoring MLP: 0.4855555555555556\n",
      "Scoring GBC: 0.4955555555555556\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean phi: {mphi}')\n",
    "print(f'Phi list: {phis}')\n",
    "print(f'Scoring MLP: {mlp.score(X_test,Y_test)}')\n",
    "print(f'Scoring GBC: {gbc.score(X_test,Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88505822",
   "metadata": {},
   "source": [
    "### Modified dataset\n",
    "Random shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90e60de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature_1  feature_2  feature_3\n",
      "2530   0.532868   0.913424        0.0\n",
      "2833   0.586248   0.929337        0.0\n",
      "2602   0.729991   0.488942        0.0\n",
      "314    0.468245   0.866721        0.0\n",
      "1767   0.369004   0.729303        0.0\n"
     ]
    }
   ],
   "source": [
    "modified_data = data[columns].copy().sample(frac = 1)\n",
    "print(modified_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3be3fd21",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Species'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Species'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8691/2847190173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodified_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodified_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Species'"
     ]
    }
   ],
   "source": [
    "modified_data['Species']= data['Species'].values.copy()\n",
    "print(modified_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_scaled_data = modified_data.copy()\n",
    "modified_scaled_data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] = scaler.fit_transform(modified_scaled_data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']])\n",
    "modified_X = modified_scaled_data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "modified_Y = modified_scaled_data['Species']\n",
    "t_r = 0.7\n",
    "\n",
    "modified_X_training = modified_X.iloc[:int(t_r*modified_X.shape[0])]\n",
    "modified_X_test = modified_X.iloc[int(t_r*modified_X.shape[0]):]\n",
    "modified_Y_training = modified_Y.iloc[:int(t_r*modified_Y.shape[0])]\n",
    "modified_Y_test = modified_Y.iloc[int(t_r*modified_Y.shape[0]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987e77d",
   "metadata": {},
   "source": [
    "### Models for modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92dd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlp = MLPClassifier(hidden_layer_sizes=(8,16,8,),random_state=1, max_iter=200)\n",
    "mgbc = GradientBoostingClassifier(n_estimators=150, learning_rate=1.0,max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b9f92",
   "metadata": {},
   "source": [
    "### Modified Dataset Information Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mphi,phis = mean_mutual_density_divergence(modified_data.iloc[:int(t_r*modified_data.shape[0])],['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'],'Species')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Mean phi: {mphi}')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Phi list: {phis}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987dea8",
   "metadata": {},
   "source": [
    "### Model Training on modified dataset\n",
    "#### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlp.fit(modified_X_training,modified_Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b77cdd",
   "metadata": {},
   "source": [
    "#### Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d137a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgbc.fit(modified_X_training,modified_Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20553a91",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b173d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean phi: {mphi}')\n",
    "print(f'Phi list: {phis}')\n",
    "print(f'Scoring MLP: {mmlp.score(X_test,Y_test)}')\n",
    "print(f'Scoring GBC: {mgbc.score(X_test,Y_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e879b",
   "metadata": {},
   "source": [
    "### Modification on dataset\n",
    "Common Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa662400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset & computer centers of mass\n",
    "categories = category_finder(training_dataset = data,category_column='Species')\n",
    "st_DataFrames = []\n",
    "for category_name in categories.keys():\n",
    "    st = StandardScaler()\n",
    "    category_array = categories[category_name]\n",
    "    category_array[columns] = st.fit_transform(category_array[columns])\n",
    "    st_DataFrames.append(category_array)\n",
    "#print(st_DataFrames)\n",
    "st_modified_data = pd.concat(st_DataFrames).sample(frac=1)\n",
    "#print(f'Standard Mofied Data:\\n {st_modified_data}')\n",
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bf3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_modified_X = st_modified_data[columns]\n",
    "st_modified_Y = st_modified_data['Species']\n",
    "t_r = 0.7\n",
    "\n",
    "st_modified_X_training = st_modified_X.iloc[:int(t_r*st_modified_X.shape[0])]\n",
    "st_modified_X_test = st_modified_X.iloc[int(t_r*st_modified_X.shape[0]):]\n",
    "st_modified_Y_training = st_modified_Y.iloc[:int(t_r*st_modified_Y.shape[0])]\n",
    "st_modified_Y_test = st_modified_Y.iloc[int(t_r*st_modified_Y.shape[0]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e726ac6",
   "metadata": {},
   "source": [
    "### Models for modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlp = MLPClassifier(hidden_layer_sizes=(8,16,8,),random_state=1, max_iter=200)\n",
    "mgbc = GradientBoostingClassifier(n_estimators=150, learning_rate=1.0,max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ad0c5",
   "metadata": {},
   "source": [
    "### Modified Dataset Information Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4aaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mphi,phis = mean_mutual_density_divergence(st_modified_data.iloc[:int(t_r*st_modified_data.shape[0])],['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'],'Species')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Mean phi: {mphi}')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Phi list: {phis}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762e7d5",
   "metadata": {},
   "source": [
    "### Model Training on modified dataset\n",
    "#### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46468b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlp.fit(st_modified_X_training,st_modified_Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91835416",
   "metadata": {},
   "source": [
    "#### Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgbc.fit(st_modified_X_training,st_modified_Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343eee8d",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean phi: {mphi}')\n",
    "print(f'Phi list: {phis}')\n",
    "print(f'Scoring MLP: {mmlp.score(X_test,Y_test)}')\n",
    "print(f'Scoring GBC: {mgbc.score(X_test,Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "control_rate = 0.15\n",
    "reference_set = np.random.random(size = (100,dim))\n",
    "control_points = np.random.random(size = (int(control_rate*reference_set.shape[0]),dim))\n",
    "\n",
    "k = 0\n",
    "test_set = np.random.random(size = (100,dim))+k\n",
    "\n",
    "\n",
    "\n",
    "m, s = average_inner_distance(target_points = control_points, data = reference_set)\n",
    "print(f'm:\\n{m}\\ns:\\n{s}')\n",
    "\n",
    "i_m = dataset_intersection_counter(target_reference_points = control_points, radius = m+s, target_dataset =  test_set)\n",
    "print(f'Intersection measure: {i_m}')\n",
    "\n",
    "f = plt.figure(figsize = (12,10))\n",
    "a = f.add_subplot(111)\n",
    "a.scatter(reference_set[:,0],reference_set[:,1],color = 'blue',label='Reference Set')\n",
    "a.scatter(control_points[:,0],control_points[:,1],color = 'red',label='Control Points')\n",
    "a.scatter(test_set[:,0],test_set[:,1],color = 'orange',label='Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caacbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
