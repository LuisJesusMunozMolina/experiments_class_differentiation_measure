{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68db98ef",
   "metadata": {},
   "source": [
    "# Cell Phone Dataset Experiment\n",
    "This notebook is aimed at testing the capabilities of the class differentiation measure for the case of a more general dataset. In this case we are going to analyse the case of a Cell Phone dataset classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67f402",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# Magics first (server issues)\n",
    "\n",
    "# if you want static matplotlib plot :\n",
    "#%matplotlib inline \n",
    "# Do below if you want interactive matplotlib plot :\n",
    "%matplotlib notebook \n",
    "\n",
    "# https://ipython.org/ipython-doc/dev/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#Customisations\n",
    "\n",
    "\n",
    "#Any tweaks that normally go in .matplotlibrc, etc. should explicity go here\n",
    "plt.rcParams['figure.figsize'] = (7,5)\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060eb0b",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e32b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_point(point,array):\n",
    "    dist = []\n",
    "    for i in range(array.shape[0]):\n",
    "        d = np.linalg.norm(point-array[i])\n",
    "        dist.append(d)\n",
    "    return np.array(dist)\n",
    "\n",
    "def distance_factor(sample,mode = 0):\n",
    "    \"\"\"\n",
    "    This function computes the relative contrast provided by the paper  \"ON the suprising behaviour\". \n",
    "    In this case, the relative constras will be computed with respect the mean.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    sample: Dataset from which the relative factor must be computed.\n",
    "    mode: working mode. Default:0\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    rt : Relative contrast.\n",
    "    \"\"\"\n",
    "    if mode == 0:\n",
    "        dt = centroid_distance(sample)\n",
    "        m = np.min(dt)\n",
    "        M = np.max(dt)\n",
    "        rt = (M-m)/m\n",
    "        return rt\n",
    "    \n",
    "\n",
    "def mahalanobis(array,observations):\n",
    "    \"\"\"\n",
    "    This function computes the Mahalanobis \"Norm\" of a provided vector array and a provided set of\n",
    "     observations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array: Provided array. This distance will provide a measure of how much this vector belongs to\n",
    "    the provided set of vectors.\n",
    "    observations: Set of previous observations.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Mahalanobis distance to the provided set.\n",
    "    \n",
    "    \"\"\"\n",
    "    observations = np.transpose(observations)\n",
    "    #print(f'Observations shape: {observations.shape}')\n",
    "    mean = np.mean(observations,axis = 1)\n",
    "    #print(f'Mean  {mean}')\n",
    "    cov = np.cov(observations)\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    #print(f'Cov: {cov}')\n",
    "    v = array - mean\n",
    "    distance = np.sqrt(np.matmul(np.matmul(v,inv_cov),v))\n",
    "    return distance\n",
    "\n",
    "def d_mahalanobis_distance(sample):\n",
    "    \"\"\"\n",
    "    This function computes the mahalanobis distance of each single point to itself\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample. samples or dataset used to compute the mahalanobis distance.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    d_v: distance vector\n",
    "    \"\"\"\n",
    "    d_v = []\n",
    "    for i in sample:\n",
    "        d_v.append(mahalanobis(i,sample))\n",
    "    d_v = np.array(d_v)\n",
    "    return d_v\n",
    "\n",
    "def centroid_distance(sample):\n",
    "    \"\"\"\n",
    "    This function computes the distance from the mass center of a given set to each other point\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    sample: sample of elements on which the method will be applied.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    d_v: distance vector\n",
    "    \"\"\"\n",
    "    m = np.mean(sample,axis=0)\n",
    "    d_v=distance_to_point(m,sample)\n",
    "    return d_v\n",
    "\n",
    "def density_funct_estimation(sample,density_factor=100):\n",
    "    \"\"\"\n",
    "    This function computes the density estimation associated to a category in a\n",
    "    dataset.\n",
    "    \"\"\"\n",
    "    q = []\n",
    "    for i in range(1,density_factor):\n",
    "        q.append(np.quantile(sample,i/density_factor))\n",
    "    q = np.array(q)\n",
    "    q = np.concatenate((np.array([np.min(sample)]),q,np.array([np.max(sample)])))\n",
    "    dq = np.diff(q)\n",
    "    h = (1/density_factor)/dq\n",
    "\n",
    "    return q[1:],h\n",
    "\n",
    "def quantile_entropy(density_vector):\n",
    "    \"\"\"\n",
    "    This function computes the entropy associated to a simple density function made out of constant values\n",
    "    obtained for equally spaciated quantiles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    density_vector: array. Vector containing the densities obtained for a specific sample.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    qe: Scalar value associated to the entropy of the sample.\n",
    "    \n",
    "    \"\"\"\n",
    "    c = 1/density_vector.shape[0]\n",
    "    qe = (-c)*np.sum(np.log(density_vector))\n",
    "    return qe\n",
    "def n_sphere_vol(r,dim = 2):\n",
    "    if dim%2 == 0:\n",
    "        k = dim/2\n",
    "        volume = ((np.pi**k)/np.math.factorial(k))*r**dim\n",
    "    elif dim%2 == 1:\n",
    "        k = dim//2\n",
    "        volume = ((2*np.math.factorial(k)*(4*np.pi)**k)/(np.math.factorial(dim)))*r**dim     \n",
    "    return volume\n",
    "\n",
    "def mutual_density(target_random_sample, non_target_random_sample,non_target_quantiles ):\n",
    "    \"\"\"\n",
    "    This function computes the empirical probability measure of the intersection of two \n",
    "    categories within a dataset.\n",
    "    \n",
    "    Params\n",
    "    --------\n",
    "    target_random_sample: np.array. Samples that belongs to the target category. The\n",
    "                          probability measure will be computed with respect the empirical\n",
    "                          probability distribution associated to this category.\n",
    "    non_target_random_sample: np.array. Samples that belong to some other category. If \n",
    "                          these two categories are not disjoint then the intersection will\n",
    "                          be a measurable set and its measure can be computed with respect\n",
    "                          the different probability measures.\n",
    "    non_target_quantiles: np.array. Array that contains the quantiles of distances with respect\n",
    "                          the non_target_random_sample.\n",
    "                          \n",
    "    Return\n",
    "    -------\n",
    "    m: float. Measure of the intersection of the two categories with respect the empirical prob\n",
    "              bability measure defined for the target_random_sample-\n",
    "    \"\"\"\n",
    "    \n",
    "    r = non_target_quantiles[-1]\n",
    "    c2 = np.mean(non_target_random_sample,axis=0)\n",
    "    distances_to_c2 = distance_to_point(point = c2,array = target_random_sample)\n",
    "    categories_intersection = distances_to_c2[np.where(distances_to_c2<=r)]\n",
    "    m = categories_intersection.shape[0]/target_random_sample.shape[0]\n",
    "    return m\n",
    "\n",
    "def intersection_divergence_entropy(dataset_1,dataset_2):\n",
    "    \"\"\"\n",
    "    Test\n",
    "    \"\"\"\n",
    "    distances1 = centroid_distance(dataset_1)\n",
    "    distances2 = centroid_distance(dataset_2)\n",
    "    q1,h1 = density_funct_estimation(distances1)\n",
    "    q2,h2 = density_funct_estimation(distances2)\n",
    "    m1 = mutual_density(dataset_1,dataset_2,q2)\n",
    "    m2 = mutual_density(dataset_2,dataset_1,q1)\n",
    "    i_d_e = m1*np.log(m1/m2)\n",
    "    return i_d_e\n",
    "\n",
    "def mutual_density_divergence(target_category, category_list):\n",
    "    \"\"\"\n",
    "    This function computes a coefficient associated to the measure of\n",
    "    the intersections of a measurable set with respect someother measurable sets \n",
    "    for a set of probability measures. This coefficient intends to provide insights \n",
    "    of the average 'size'of these intersections and, in that way, how distinguishable\n",
    "    is a set from others.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    target_category: array. Dataset associated to the target category for which the coe-\n",
    "                     fficient will be computed.\n",
    "    category_list: list of arrays. This list must contain all datasets associated to the \n",
    "              different categories  in a classification problem.\n",
    "    mode: int. Used mean method. Default:0 Arithmetic mean. 1, geometric mean.\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    phi: float. Coefficient associated to the target category.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    relative_measures = []\n",
    "    for category in category_list:\n",
    "        d = centroid_distance(category)\n",
    "        q,h = density_funct_estimation(d)\n",
    "        m=mutual_density(target_category,category,q)\n",
    "        relative_measures.append(m)\n",
    "    relative_measures = np.array(relative_measures)\n",
    "    phi = np.mean(np.log(1/(1-relative_measures)))\n",
    "    return phi, relative_measures\n",
    "\n",
    "def dataset_density_function_estimation(dataset):\n",
    "    d = centroid_distance(dataset)\n",
    "    q,h = density_funct_estimation(d)\n",
    "    return q,h\n",
    "\n",
    "def dataset_quantile_entropy(dataset):\n",
    "    q,h = dataset_density_function_estimation(dataset)\n",
    "    e = quantile_entropy(h)\n",
    "    return e\n",
    "def mean_categorical_entropy(category_list):\n",
    "    \"\"\"\n",
    "    This function computes the mean categorical entropy defined for n categories in\n",
    "    a classification problem as (phi1*h1+...+phin*hn)/n\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    category_list:list of arrays. This list must contain all datasets associated to the \n",
    "              different categories  in a classification problem.\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    mce:float. Mean Cathegorical Entropy. \n",
    "    \"\"\"\n",
    "    entropies = []\n",
    "    mdd_coefficients=[]\n",
    "    \n",
    "    for i in  range(category_list.shape[0]):\n",
    "        quantile_entropy = dataset_quantile_entropy(category_list[i])\n",
    "        entropies.append(quantile_entropy)\n",
    "        target_category = category_list[i]\n",
    "        non_target_category = np.delete(category_list,i,axis = 0)\n",
    "        #print(f'shape {non_target_category.shape}')\n",
    "        phi,_relative_measures = mutual_density_divergence(target_category = target_category,category_list = non_target_category)\n",
    "        print(f'Relative Measures {_relative_measures}')\n",
    "        print(f'Coefficients {phi}')\n",
    "        mdd_coefficients.append(phi)\n",
    "    entropies = np.array(entropies)\n",
    "    mdd_coefficients = np.array(mdd_coefficients)    \n",
    "    mce = np.mean(100*np.log2(mdd_coefficients)+entropies)\n",
    "    return mce\n",
    "\n",
    "\n",
    "\n",
    "def category_finder(training_dataset,category_column):\n",
    "    \"\"\"\n",
    "    This function takes a pandas DataFrame training_dataset as \n",
    "    input and returns a set of pandas DataFrame each associated \n",
    "    to a specific category\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_dataset: Pandas DataFrame. DataFrame that contains the\n",
    "                      training dataset\n",
    "    category_columns: str. Name of the column that contains the \n",
    "    categories\n",
    "    \n",
    "    Returns: category_dict. Dictionary that contains the different\n",
    "            categories dataframes\n",
    "    \"\"\"\n",
    "    category_dict = {}\n",
    "    categories = training_dataset[category_column].unique()\n",
    "    #print(f'Unique Values: {categories}')\n",
    "    for i in range(categories.shape[0]):\n",
    "        #print(f'Categoria: {categories[i]}')\n",
    "        name = categories[i]\n",
    "        current_df = training_dataset.loc[training_dataset[category_column]==categories[i]].copy()\n",
    "        category_dict[name]= current_df\n",
    "    return category_dict\n",
    "\n",
    "def mean_mutual_density_divergence(dataset,representation_columns,category_column):\n",
    "    \"\"\"\n",
    "    this function computes the mean mutual density divergence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: pandas DataFrame. dataset that is going to be used for training the model (\n",
    "             all columns must be included)\n",
    "    representation_columns: list of str. Names of the columns that stand for the representation,\n",
    "                          i.e. the feature columns\n",
    "    category_column: str. Name of the column that contains the categories\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    mean_phi: float. Mean of the phi values associated to the different mutual density divergence\n",
    "    \n",
    "    phis: list of float. List of phi values associated to the different mutual density divergence\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    categories = category_finder(dataset,category_column)\n",
    "    categories_names = list(categories.keys())\n",
    "    phis = []\n",
    "    for i in range(len(categories_names)):\n",
    "        remaining_category_names = categories_names.copy()\n",
    "        target_category_name = categories_names[i]\n",
    "        remaining_category_names.remove(target_category_name)\n",
    "        #print(f'Target Category Name: {target_category_name}')\n",
    "        #print(f'Remaining Category Name: {remaining_category_names}')\n",
    "        #computing the divergence\n",
    "        target_X = categories[target_category_name][representation_columns].values\n",
    "        #print(f'This is target_X: { target_X}')\n",
    "        complementary_targets = [categories[name][representation_columns].values for name in remaining_category_names]\n",
    "        #print(f'These are complementary targets: {complementary_targets}')\n",
    "        \n",
    "        phi,rm = mutual_density_divergence(target_category=target_X, category_list=complementary_targets)\n",
    "        phis.append(phi)\n",
    "        #print(f'Mean phi:{np.mean(phis)}\\nList of phis: {phis}')\n",
    "        \n",
    "        del remaining_category_names\n",
    "    return np.mean(phis),phis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba665f4",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('./datasets/cell_phone_train.csv')\n",
    "p_test = Path('./datasets/cell_phone_test.csv')\n",
    "scaler = MinMaxScaler()\n",
    "#columns = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b63025",
   "metadata": {},
   "source": [
    "### Control Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(p).sample(frac=1)\n",
    "test_data = pd.read_csv(p_test)\n",
    "print(data.head())\n",
    "columns = list(data.columns)[:-1]\n",
    "target_column = list(data.columns)[-1]\n",
    "print(f'Data Shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7135f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaled_data = data.copy()\n",
    "scaled_data[columns] = scaler.fit_transform(scaled_data[columns])\n",
    "X = scaled_data[columns]\n",
    "Y = scaled_data[target_column]\n",
    "t_r = 0.7\n",
    "\n",
    "X_training = X.iloc[:int(t_r*X.shape[0])]\n",
    "X_test = X.iloc[int(t_r*X.shape[0]):]\n",
    "Y_training = Y.iloc[:int(t_r*Y.shape[0])]\n",
    "Y_test = Y.iloc[int(t_r*Y.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Data: {data.head()}\\nX:{X.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad7f8b",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(32,64,64,16),random_state=1, max_iter=200)\n",
    "gbc = GradientBoostingClassifier(n_estimators=150, learning_rate=1.0,max_depth=1, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da130493",
   "metadata": {},
   "source": [
    "### Control Dataset information analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb31287",
   "metadata": {},
   "outputs": [],
   "source": [
    "mphi,phis = mean_mutual_density_divergence(data.iloc[:int(t_r*data.shape[0])],columns,target_column)\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Mean phi: {mphi}')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Phi list: {phis}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118c640",
   "metadata": {},
   "source": [
    "### Model Training on Control Dataset\n",
    "#### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313aba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_training,Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03640103",
   "metadata": {},
   "source": [
    "#### Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.fit(X_training,Y_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa0eaf",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c75036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean phi: {mphi}')\n",
    "print(f'Phi list: {phis}')\n",
    "print(f'Scoring MLP: {mlp.score(X_test,Y_test)}')\n",
    "print(f'Scoring GBC: {gbc.score(X_test,Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88505822",
   "metadata": {},
   "source": [
    "### Modified dataset\n",
    "Random shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e60de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_data = data[columns].copy().sample(frac = 1)\n",
    "print(modified_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_data['Species']= data['Species'].values.copy()\n",
    "print(modified_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_scaled_data = modified_data.copy()\n",
    "modified_scaled_data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] = scaler.fit_transform(modified_scaled_data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']])\n",
    "modified_X = modified_scaled_data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "modified_Y = modified_scaled_data['Species']\n",
    "t_r = 0.7\n",
    "\n",
    "modified_X_training = modified_X.iloc[:int(t_r*modified_X.shape[0])]\n",
    "modified_X_test = modified_X.iloc[int(t_r*modified_X.shape[0]):]\n",
    "modified_Y_training = modified_Y.iloc[:int(t_r*modified_Y.shape[0])]\n",
    "modified_Y_test = modified_Y.iloc[int(t_r*modified_Y.shape[0]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987e77d",
   "metadata": {},
   "source": [
    "### Models for modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92dd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlp = MLPClassifier(hidden_layer_sizes=(8,16,8,),random_state=1, max_iter=200)\n",
    "mgbc = GradientBoostingClassifier(n_estimators=150, learning_rate=1.0,max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b9f92",
   "metadata": {},
   "source": [
    "### Modified Dataset Information Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mphi,phis = mean_mutual_density_divergence(modified_data.iloc[:int(t_r*modified_data.shape[0])],['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'],'Species')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Mean phi: {mphi}')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Phi list: {phis}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987dea8",
   "metadata": {},
   "source": [
    "### Model Training on modified dataset\n",
    "#### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlp.fit(modified_X_training,modified_Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b77cdd",
   "metadata": {},
   "source": [
    "#### Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d137a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgbc.fit(modified_X_training,modified_Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20553a91",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b173d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean phi: {mphi}')\n",
    "print(f'Phi list: {phis}')\n",
    "print(f'Scoring MLP: {mmlp.score(X_test,Y_test)}')\n",
    "print(f'Scoring GBC: {mgbc.score(X_test,Y_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e879b",
   "metadata": {},
   "source": [
    "### Modification on dataset\n",
    "Common Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa662400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset & computer centers of mass\n",
    "categories = category_finder(training_dataset = data,category_column='Species')\n",
    "st_DataFrames = []\n",
    "for category_name in categories.keys():\n",
    "    st = StandardScaler()\n",
    "    category_array = categories[category_name]\n",
    "    category_array[columns] = st.fit_transform(category_array[columns])\n",
    "    st_DataFrames.append(category_array)\n",
    "#print(st_DataFrames)\n",
    "st_modified_data = pd.concat(st_DataFrames).sample(frac=1)\n",
    "#print(f'Standard Mofied Data:\\n {st_modified_data}')\n",
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bf3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_modified_X = st_modified_data[columns]\n",
    "st_modified_Y = st_modified_data['Species']\n",
    "t_r = 0.7\n",
    "\n",
    "st_modified_X_training = st_modified_X.iloc[:int(t_r*st_modified_X.shape[0])]\n",
    "st_modified_X_test = st_modified_X.iloc[int(t_r*st_modified_X.shape[0]):]\n",
    "st_modified_Y_training = st_modified_Y.iloc[:int(t_r*st_modified_Y.shape[0])]\n",
    "st_modified_Y_test = st_modified_Y.iloc[int(t_r*st_modified_Y.shape[0]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e726ac6",
   "metadata": {},
   "source": [
    "### Models for modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlp = MLPClassifier(hidden_layer_sizes=(8,16,8,),random_state=1, max_iter=200)\n",
    "mgbc = GradientBoostingClassifier(n_estimators=150, learning_rate=1.0,max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ad0c5",
   "metadata": {},
   "source": [
    "### Modified Dataset Information Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4aaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mphi,phis = mean_mutual_density_divergence(st_modified_data.iloc[:int(t_r*st_modified_data.shape[0])],['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'],'Species')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Mean phi: {mphi}')\n",
    "print('---------------------------------\\n---------------------------------')\n",
    "print(f'Phi list: {phis}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762e7d5",
   "metadata": {},
   "source": [
    "### Model Training on modified dataset\n",
    "#### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46468b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlp.fit(st_modified_X_training,st_modified_Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91835416",
   "metadata": {},
   "source": [
    "#### Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgbc.fit(st_modified_X_training,st_modified_Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343eee8d",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean phi: {mphi}')\n",
    "print(f'Phi list: {phis}')\n",
    "print(f'Scoring MLP: {mmlp.score(X_test,Y_test)}')\n",
    "print(f'Scoring GBC: {mgbc.score(X_test,Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1dec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
